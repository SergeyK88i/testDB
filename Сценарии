Финальный мастер-шаблон (версия с полными подсказками)

  1. Идентификация и Цель
   * Ключевые вопросы:
       * Какую бизнес-проблему мы решаем?
       * Какая система является источником данных?
       * Какие данные нам нужны на концептуальном уровне?
       * С кем со стороны источника велось общение? (контакт для технических вопросов)

  2. Инфраструктура и Окружение

   * 2.1. Сетевое взаимодействие (Маршрут и Доступы)
       * Ключевые вопросы:
           * Документация архитектуры: Отражена ли интеграция в официальной архитектурной схеме проекта?
           * Местоположение: В каких сегментах сети находятся наш сервис и целевая система?
           * Маршрут и Точка подключения: Исходя из местоположения, каким будет маршрут (прямой, через proxy/gateway)? Каким будет итоговый
             адрес для подключения нашего сервиса?
           * Процесс согласования: Требуется ли формальное согласование этого сетевого взаимодействия (например, с отделом ИБ)?
           * Запрос на доступ: Если да, что именно нужно указать в заявке на открытие доступов (IP источника, IP назначения, порт)?

   * 2.2. Тестовые среды
       * Ключевые вопросы:
           * Есть ли у источника тестовый контур? Каковы его реквизиты и ограничения?

   * 2.3. Готовность на PROD
       * Ключевые вопросы:
           * Нужный функционал выведен и стабильно работает в PROD-среде источника если нет заведен  CR ссылку на CR?

  3. Доступ к Источнику (Прикладной уровень)

   * 3.1. Технология и паттерн интеграции
       * Ключевые вопросы:
           * Какая технология и паттерн используются (e.g., REST, gRPC, Kafka, DB Connection)?

   * 3.2. Реквизиты для подключения (Адреса и Аутентификация)
       * Ключевые вопросы (адаптируются под п. 3.1):
           * Каковы адреса/эндпоинты для подключения?
           * Какой механизм аутентификации используется?
           * Какие конкретно креды/ключи/сертификаты нужны?
       * Подсказка/Примеры:
           * Адреса:
               * Для REST: https://api.some-service.com/v1/
               * Для Kafka: kafka1.corp.local:9092
               * Для БД: postgres-db.prod.local:5432
           * Механизмы аутентификации:
               * Bearer Token, API Key, Basic Auth, mTLS, SASL/SCRAM, AD ТУЗ (Kerberos/NTLM)
           * Креды/ключи (конкретные секреты):
               * Для API Key: ABC-123-XYZ-456
               * Для mTLS: Указание на файлы client.crt, client.key и их Common Name (CN).
               * Для AD ТУЗ: Пара логин/пароль. Например: login: srv_myapp_prod, password: (хранится в vault).

   * 3.3. Процесс получения доступа (Бюрократия)
       * Ключевые вопросы:
           * Есть ли документация, описывающая процесс получения доступа? (e.g., статья в Confluence, регламент)
           * Каковы точные шаги для получения кредов/ключей/сертификатов, описанных в п. 3.2? (e.g., "Подать заявку в Service Desk по шаблону
             X")

  4. Логика Работы с Данными
   * Ключевые вопросы:
       * Где находится документация на API / формат сообщений?
       * Какой эндпоинт/топик/таблицу мы используем?
       * Какие параметры запроса нужно передавать / как фильтровать сообщения?
       * Какие поля из ответа/сообщения нам нужны и какую логику к ним применить?
       * Каков финальный результат вычисления?

  5. Ускорение Разработки и Критерии Успеха
   * Ключевые вопросы:
       * Как выглядят примеры ответов/сообщений (успех, ошибка, пустой ответ)?
       * Как мы однозначно поймем, что задача выполнена корректно?

  Обновленный раздел 6

  6. Эксплуатация и Ограничения

   * 6.1. Ограничения на стороне источника
       * Ключевые вопросы:
           * Есть ли ограничения на частоту запросов (rate limits)?
           * Какой должна быть наша стратегия повторных запросов (retry policy)?

   * 6.2. Метрики и Аудит
       * Ключевые вопросы:
           * Технические метрики: Какие технические метрики нужно собирать для мониторинга здоровья интеграции? (e.g., количество 
             успехов/ошибок, время выполнения)
           * Данные для аудита: Какие данные о результате вычисления и "доказательства" нужно сохранять для последующего аудита и разбора?
             (e.g., "Сохранять итоговый балл, ID проверенной страницы в Confluence, время проверки")

  ---

  Пример заполнения для нового пункта 6.2

   * Технические метрики:
       * счетчик успешных проверок по критерию
       * счетчик ошибок по критерию и типу (сетевая, ошибка парсинга, нет данных)
       * гистограмма длительности выполнения проверки (latency)
   * Данные для аудита:
       * ID источника
       * ID критерия
       * итоговый балл
       * время проверки (timestamp)
       * ключевое "доказательство" (например, ID найденной страницы в Confluence или конкретное значение, полученное из API, которое повлияло 
         на решение)

1. Технические метрики (для инженеров)

  Цель: Следить за "здоровьем" и производительностью интеграции в реальном времени. Отвечают на вопрос: "Все ли работает как надо?". Эти
  данные обычно отправляются в системы мониторинга (например, Prometheus, Grafana).

   * `счетчик успешных проверок по критерию`
       * Что это: Простая цифра, которая увеличивается на 1 каждый раз, когда мы успешно рассчитали один критерий для одного источника.
       * Зачем это нужно: Если мы видим, что этот счетчик перестал расти, значит, наш процесс сломался или остановился. Мы можем настроить
         алерт: "Если за час не было ни одной успешной проверки, прислать уведомление в #alerts".

   * `счетчик ошибок по критерию и типу (сетевая, ошибка парсинга, нет данных)`
       * Что это: Счетчик, который срабатывает при ошибке. Важно, что мы помечаем его "типом" ошибки.
       * Зачем это нужно: Это главный инструмент для диагностики.
           * Резко вырос счетчик сетевых ошибок? Значит, проблема со связью до системы-источника, нужно проверить сеть.
           * Растет счетчик ошибок парсинга? Значит, источник мог поменять формат ответа своего API, и нам нужно адаптировать наш код.
           * Растет счетчик "нет данных"? Значит, интеграция работает, но мы не находим нужную информацию, возможно, ее удалили или
             переместили.

   * `гистограмма длительности выполнения проверки (latency)`
       * Что это: Метрика, которая измеряет, сколько миллисекунд или секунд уходит на одну проверку. Гистограмма позволяет видеть не просто
         среднее время, а распределение (e.g., 90% запросов выполняются за 200мс, а 10% — за 2 секунды).
       * Зачем это нужно: Для отслеживания производительности. Если мы видим, что время проверки постепенно растет, это сигнал, что
         система-источник стала медленнее или наш алгоритм неэффективен. Это помогает обеспечивать соблюдение нефункциональных требований
         ("система должна быть быстрой").

  ---

  2. Данные для аудита (для бизнес-пользователей и владельцев источников)

  Цель: Обеспечить полную прозрачность и доказуемость расчетов. Отвечают на вопрос: "Почему для этого источника получился именно такой
  балл?". Эти данные обычно сохраняются в базу данных или лог-файлы на длительный срок.

   * `ID источника`, `ID критерия`, `итоговый балл`, `время проверки (timestamp)`
       * Что это: Базовая информация: кто, что проверяли, какой результат получили и когда.
       * Зачем это нужно: Это основа для любого отчета или расследования. Позволяет делать запросы вроде "Покажи все расчеты для 'Источника А'
         за последний месяц".

   * `ключевое "доказательство"`
       * Что это: Это самая важная часть для аудита. Это конкретная улика или факт, на основе которого был сделан вывод.
       * Зачем это нужно: Это ответ на вопрос "На каком основании?".
           * Пример 1: Критерий — "Наличие документации в Confluence". Расчет успешен, балл = 10. "Доказательством" будет page_id=54321
             найденной страницы. Если владелец источника спросит "Почему 10?", мы ответим: "Потому что мы нашли вот эту страницу: [ссылка]".
           * Пример 2: Критерий — "Отсутствие критичных инцидентов". Расчет провален, балл = 0. "Доказательством" будет incident_id='INC-98765'
             найденного инцидента. Мы можем сказать: "Балл снижен из-за этого конкретного инцидента, разберитесь с ним".


-------------------
Цель мониторинга — в реальном времени, с помощью дашбордов (в Grafana), понимать, что происходит с вашей системой. Это приборная панель
  вашего сервиса.

  Принято использовать стандартный набор метрик, который описывает три ключевых аспекта работы любого сервиса (метод RED: Rate, Errors,
  Duration).

  Вот что должно быть в вашем мониторинге для каждой интеграции:

  ---

  1. Rate (Интенсивность) — "Как часто мы это делаем?"

  Эта метрика показывает нагрузку на вашу систему.

   * Название метрики: integration_runs_total
   * Тип: Counter (счетчик, который только увеличивается)
   * Зачем нужна:
       * Показывает, сколько всего проверок было запущено.
       * Если график rate(integration_runs_total[5m]) (скорость запусков за 5 минут) падает до нуля, это значит, что ваш сервис перестал
         запускать проверки. Это повод для алерта.
       * Резкий всплеск может говорить о том, что проверки запускаются по ошибке слишком часто.
   * Важные метки/лейблы: criterion_id (чтобы видеть интенсивность по каждому критерию отдельно).

  2. Errors (Ошибки) — "Как много у нас проблем?"

  Это самая важная метрика для оценки здоровья системы.

   * Название метрики: integration_errors_total
   * Тип: Counter
   * Зачем нужна:
       * Показывает общее количество ошибок. Если это число начинает расти — это главный сигнал о проблеме и повод для срочного алерта.
       * Позволяет считать уровень ошибок (Error Rate) = количество ошибок / общее количество запусков.
   * Важные метки/лейблы:
       * criterion_id: Позволяет понять, какой именно критерий "сломался".
       * error_type: Критически важная метка для диагностики. Значения могут быть:
           * network: Не удалось подключиться к источнику.
           * auth: Ошибка аутентификации (неверный токен, пароль).
           * parsing: Не удалось разобрать ответ от API (источник поменял формат).
           * business_logic: Ошибка в нашей внутренней логике расчета.

  Как это используется: На дашборде в Grafana вы строите график скорости роста ошибок, сгруппированный по error_type. Если вы видите всплеск
  ошибок типа auth для критерия C-05, вы точно знаете, что у вас, скорее всего, "протух" токен для этой конкретной интеграции.

  3. Duration (Длительность) — "Как быстро мы работаем?"

  Эта метрика отслеживает производительность и "тормоза".

   * Название метрики: integration_duration_seconds
   * Тип: Histogram или Summary
   * Зачем нужна:
       * Показывает, сколько времени занимает расчет одного критерия.
       * Помогает отслеживать деградацию производительности. Если среднее время или 99-й перцентиль (самые медленные запросы) начинают расти,
         это значит, что либо источник стал медленнее отвечать, либо наш код стал менее эффективным.
       * Позволяет настроить алерт: "Если 95-й перцентиль времени ответа больше 5 секунд, прислать уведомление".
   * Важные метки/лейблы: criterion_id.

  ---

  Итог для дашборда

  Имея эти три типа метрик, вы можете создать в Grafana полный дашборд для мониторинга ваших интеграций, где по каждому критерию будет видно:
   1. График интенсивности запусков.
   2. График количества и типа ошибок.
   3. График времени выполнения (среднее и 99-й перцентиль).

  Это позволяет вам перейти от реактивного подхода ("пользователь сообщил о проблеме") к проактивному ("наш дашборд показывает аномалию,
  давайте исправим ее до того, как пользователи это заметят").

=============================================================

Да, вы можете начать декомпозицию и разработку раньше. Для этого нужно разделить наш шаблон на два блока: абсолютный минимум для старта и
  все остальное, что можно получить параллельно.

  ---

  Блок 1: Абсолютный минимум для старта разработки (пишем код на моках)

  Это тот набор информации, без которого разработчик в принципе не может начать писать код, даже в изоляции от реальной системы. Как только
  аналитик заполнил эти пункты, техлид может создавать первые задачи.

  Супер обязательные пункты:

   1. Раздел 1. Идентификация и Цель (полностью): Нужно понимать бизнес-задачу.
   2. Пункт 3.1. Технология и паттерн интеграции: Принципиально важно знать, это REST или Kafka, так как от этого зависит вся архитектура кода.
   3. Раздел 4. Логика Работы с Данными (частично, но критично):
       * Структура данных: Какие поля есть в ответе/сообщении.
       * Основная логика: Что с этими полями делать.
       * Ссылка на документацию может подождать, если есть примеры.
   4. Пункт 5. Ускорение Разработки (только примеры ответов/сообщений): Это ключ к параллельной разработке. Имея на руках пример JSON-ответа,
      разработчик может создать классы, написать логику и полностью покрыть ее юнит-тестами, не имея реального доступа к системе.

  Что можно делать с этим блоком?
  Как только эти пункты заполнены, техлид может ставить задачи вида:
   * "Создать модель данных (DTO) для ответа от Системы Х".
   * "Реализовать сервис для обработки данных от Системы Х (на входе мок, на выходе результат)".
   * "Написать юнит-тесты для сервиса обработки".

  ---

  Блок 2: Необходимо для реального подключения и вывода в PROD (заполняется параллельно)

  Эти пункты не блокируют написание основной бизнес-логики, но абсолютно необходимы для того, чтобы интеграция заработала по-настоящему.
  Аналитик может продолжать "добывать" эту информацию, пока разработчики пишут код на основе данных из Блока 1.

  Пункты, которые могут подождать (но их получение нужно запустить как можно раньше):

   1. Раздел 2. Инфраструктура и Окружение (полностью): Все вопросы про сеть, доступы, стенды. Это часто самые долгие процессы, их нужно
      запускать в параллель.
   2. Раздел 3 (кроме п. 3.1): Все, что касается реальных адресов, реквизитов и бюрократии по получению доступов.
   3. Раздел 6. Эксплуатация и Ограничения (полностью): Ретраи, метрики, лимиты. Эту "обвязку" можно добавить к уже написанному коду основной
      логики.
   4. Критерии успеха из Раздела 5: Их можно финализировать чуть позже.

  Итог: Стратегия параллельной работы

   1. Аналитик в первую очередь фокусируется на получении информации для Блока 1.
   2. Как только Блок 1 готов, он передает его техлиду. Разработка начинается.
   3. Параллельно аналитик (или техлид) запускает все процессы по получению информации и доступов для Блока 2 (подает заявки в ИБ, запрашивает
      креды и т.д.).
   4. Когда разработчик заканчивает код на моках, в идеале, доступы из Блока 2 уже получены. Остается только подставить реальные адреса и креды,
      дописать логику обработки ошибок, ретраев, метрик и провести интеграционное тестирование.

===========
Оптимизированный процесс (более гибкий)

   1. Идея (Эпик от ПО).
   2. Встреча №1: Kick-off. ПО объясняет цель команде.
   3. Анализ. Аналитик работает над шаблоном, но не в изоляции, а по нашей "Инструкции" (т.е. проводит рабочую встречу с командой источника).
   4. Встреча №2 (расширенная): Воркшоп по аналитике и декомпозиции. (Объединяем ваши встречи №2 и №3)
       * Часть 1 (15-20 мин): Аналитик презентует команде заполненный шаблон. Все задают вопросы.
       * Часть 2 (30-40 мин): Техлид и разработчики сразу же, на этой же встрече, накидывают и обсуждают основные задачи в Jira. Это
         превращается в сессию груминга/планирования бэклога.
   5. Разработка (включая Unit/интеграционные тесты). Разработчики пишут код и тесты к нему.
   6. Тестирование (QA). QA-инженер проводит E2E и исследовательское тестирование.
   7. ПСИ.
   8. Релиз на PROD.
============
Информация для первоначального анализа и настройки:

   1. Контактное лицо: Получить имя и контакты технического специалиста, который будет отвечать на вопросы.

   2. Документация: Получить ссылки на всю релевантную документацию:
       * Описание API или формата сообщений.
       * Инструкции по получению доступов.

   3. Подтверждение по инфраструктуре: Получить ответы на вопросы:
       * О сетевой архитектуре.
       * О наличии или отсутствии тестовых стендов.
       * О готовности функционала в PROD-среде.

   4. Подтверждение процесса доступа: Узнать и зафиксировать точные шаги, которые нужно будет предпринять для получения доступов (как для сети,
      так и для приложения).

   5. Технические детали подключения: Узнать:
       * Точные адреса/эндпоинты/bootstrap-серверы.
       * Используемый механизм аутентификации.
       * Информацию об ограничениях (rate limits) и рекомендуемой политике повторных запросов (retry policy).

   6. Примеры данных (моки): Получить примеры реальных ответов API или сообщений для разных сценариев (успех, ошибка, пустой ответ).

==============
Шаблон спецификации для новой фичи

  1. Общая информация

   * Название фичи: (Краткое, понятное название, e.g., "Экран модерации расчетов ИЗИ")
   * Ссылка на эпик в Jira: (Ссылка на родительскую задачу)
   * Проблема / Бизнес-цель: (Описать, какую проблему пользователя или бизнеса мы решаем. ЗАЧЕМ мы это делаем? E.g., "Модераторы тратят много
     времени на проверку расчетов, так как у них нет единого интерфейса для просмотра и утверждения. Цель — ускорить процесс модерации в 3
     раза.")
   * Пользовательские роли: (Для КОГО эта фича? E.g., "Модератор", "Администратор")

  2. Функциональные требования в формате User Stories

  (Это основной раздел, описывающий, ЧТО должна делать фича. Лучше всего разбить большую фичу на несколько маленьких историй.)

   * User Story 1: Просмотр списка расчетов
       * Как Модератор, я хочу видеть на экране список всех расчетов, ожидающих модерации, чтобы я мог быстро оценить объем работы.
   * User Story 2: Фильтрация списка
       * Как Модератор, я хочу иметь возможность фильтровать список расчетов по источнику и дате, чтобы я мог найти конкретный расчет.
   * User Story 3: Утверждение/отклонение расчета
       * Как Модератор, я хочу иметь возможность нажать кнопки "Утвердить" или "Отклонить" для каждого расчета, чтобы я мог завершить процесс
         модерации.

  3. Требования к UI/UX (Пользовательский интерфейс)

  (Этот раздел отвечает на вопрос, КАК это должно выглядеть. Он не требует от аналитика быть дизайнером, но требует описать основные
  элементы.)

   * Расположение в интерфейсе: (Где пользователь найдет эту фичу? E.g., "Новый пункт меню 'Модерация' в левой боковой панели.")
   * Основные элементы экрана: (Что пользователь увидит на странице?)
       * Таблица с расчетами. Колонки: ID Источника, Дата расчета, Итоговый балл, Статус.
       * Блок с фильтрами над таблицей: выпадающий список Источник, поле для выбора Даты.
       * В каждой строке таблицы — кнопки Утвердить и Отклонить.
   * Ссылка на макеты/прототипы: (Критически важный пункт!) Ссылка на дизайн в Figma, Balsamiq или даже просто скриншот с нарисованными от руки
     элементами.

  4. Бизнес-логика и правила валидации

  (Здесь описываются конкретные правила и условия.)

   * Кнопка "Утвердить" активна только для расчетов в статусе "Ожидает модерации".
   * При отклонении расчета должно появляться модальное окно с обязательным полем для ввода причины отклонения.
   * Пользователи с ролью "Администратор" видят все расчеты, а "Модератор" — только те, что назначены на его отдел.

  5. Пограничные случаи и обработка ошибок

  (Что будет, если что-то пойдет не так или данных нет?)

   * Пустое состояние: Что показывать, если список расчетов на модерацию пуст? (e.g., "Показывать текст 'Расчетов для модерации нет' и
     картинку.")
   * Ошибка загрузки: Что делать, если не удалось загрузить список с сервера? (e.g., "Показать сообщение об ошибке 'Не удалось загрузить
     данные' и кнопку 'Попробовать снова'.")

  6. Критерии приемки (Acceptance Criteria)

  (Как мы поймем, что фича работает правильно? Формат GIVEN-WHEN-THEN идеален для QA.)

   * Сценарий 1: Успешное утверждение
       * GIVEN: Модератор находится на экране модерации и видит расчет со статусом "Ожидает модерации".
       * WHEN: Он нажимает кнопку "Утвердить".
       * THEN: Статус расчета меняется на "Утвержден", и он пропадает из списка.
   * Сценарий 2: Отклонение с причиной
       * GIVEN: Модератор нажимает кнопку "Отклонить".
       * WHEN: Он вводит причину в модальном окне и нажимает "Подтвердить".
       * THEN: Статус расчета меняется на "Отклонен", и он пропадает из списка.

  7. Нефункциональные требования

  (Требования к качеству, безопасности, производительности.)

   * Производительность: Список расчетов должен загружаться не дольше 2 секунд.
   * Безопасность: Доступ к этому экрану должен быть только у ролей "Модератор" и "Администратор".
   * Аудит: Каждое действие (утверждение, отклонение) должно записываться в журнал аудита с указанием, какой модератор и когда это сделал.
=====================
Роль аналитика здесь вырастает до системного аналитика или архитектора. Его первая задача — не детализация, а декомпозиция самого проекта.
  Он не использует другой шаблон, а создает новый, высокоуровневый документ, который можно назвать "Дорожная карта проекта" или
  "Архитектурное видение".

  Цель этого документа — разбить гигантскую задачу "внедрить Auth" на понятные, управляемые части (эпики), к каждой из которых потом уже 
  можно будет применить наши существующие шаблоны.

  Вот как может выглядеть структура такого документа:

  ---

  Шаблон: Дорожная карта проекта "Внедрение системы Auth"

  1. Цели и Область применения (Scope)
   * Бизнес-цели: Зачем мы это делаем? (e.g., "Повысить безопасность", "Упростить управление доступами", "Ввести единую точку входа (SSO)").
   * В Scope: Что входит в рамки проекта? (e.g., "Новый сервис аутентификации, интеграция с корпоративным AD, миграция 3-х ключевых
     приложений").
   * Вне Scope: Что мы НЕ делаем? (e.g., "Миграция всех 100+ приложений компании", "Управление правами внутри самих приложений").

  2. Архитектура верхнего уровня
   * Простая диаграмма, показывающая, как новый сервис Auth будет взаимодействовать с другими системами: пользователями, вашими приложениями,
     базой данных пользователей (e.g., Active Directory), и т.д.

  3. Основные этапы / Рабочие потоки (Workstreams)

  (Это ключевая часть. Здесь мы разбиваем весь проект на крупные блоки работ).

   * Этап 1: Интеграция с поставщиком учетных записей (Active Directory).
       * Тип работы: Интеграция.
       * Результат: Заполненный "Шаблон для интеграции" для подключения к AD по протоколу LDAP или через API.

   * Этап 2: Разработка базового сервиса аутентификации.
       * Тип работы: Разработка внутренней фичи (новый микросервис).
       * Результат: Заполненный "Шаблон для новой фичи", описывающий API самого сервиса Auth (например, эндпоинты /login, /token, /validate).
         Баз данных и прочего он не касается.

   * Этап 3: Разработка UI для управления пользователями и ролями.
       * Тип работы: Разработка UI-фичи.
       * Результат: Заполненный "Шаблон для новой фичи" для админ-панели, где можно будет создавать пользователей, назначать им роли и т.д.

   * Этап 4: Автоматизация создания "проектов" и заявок.
       * Тип работы: Оркестрация процессов (как в вашем предыдущем вопросе).
       * Результат: Заполненный "Шаблон для новой фичи" (для UI) и несколько "Шаблонов для интеграции" (для API Service Desk, API GitLab и
         т.д.).
=================================
===========================
Мыслительный алгоритм техлида при чтении спецификации

  Представьте, что техлид надевает две разные "шляпы" по очереди.

  Проход 1: "Шляпа пессимиста" (Анализ рисков — ваш подход)

  На этом этапе техлид читает документ и задает себе один главный вопрос: "Что может пойти не так или занять вечность?". Он ищет "белые
  пятна" и зависимости.

   * Вопрос 1: "Что нам нужно от других людей/команд?" (Внешние зависимости)
       * Он смотрит на разделы 2 и 3.
       * Находит: "Нужно согласование с ИБ", "Нужно сгенерировать сертификат", "Нужно, чтобы команда Х зарегистрировала наш сертификат".
       * Результат: Немедленно создает в Jira задачи типа [Admin] или [Infra].
           * Task: [Admin] Согласовать сетевые доступы с ИБ.
           * Task: [Infra] Развернуть proxy-сервер.
           * Вывод: Эти задачи — самые долгие, их нужно запустить первыми, чтобы они выполнялись параллельно с разработкой.

   * Вопрос 2: "В чем мы не уверены?" (Техническая неопределенность)
       * Он смотрит на раздел 3.1 и 3.2.
       * Находит: "Используется протокол gRPC, с которым команда еще не работала" или "Механизм аутентификации — проприетарный".
       * Результат: Создает исследовательские задачи типа [Spike].
           * Spike: Исследовать подключение к gRPC-сервису и сгенерировать клиент.
           * Вывод: Эти задачи нужно сделать до основной разработки, чтобы понять, как вообще подступиться к технологии.

   * Вопрос 3: "Что мешает разработчику начать писать код прямо сейчас?" (Блокеры разработки)
       * Он смотрит на раздел 5.
       * Находит: "Примеры ответов API отсутствуют".
       * Результат: Создает блокирующую задачу для аналитика.
           * Task: [АНАЛИТИКА] Добавить в спецификацию моки ответов от API.
           * Вывод: Без этого разработчик не сможет написать код, который можно проверить.

  Проход 2: "Шляпа архитектора" (Декомпозиция работы)

  После того как все риски и "дыры" выявлены и по ним созданы задачи, техлид делает второй проход. Теперь он задает вопрос: "Из каких 
  технических частей состоит эта фича?". Здесь он использует ту самую классификацию, которую мы обсуждали.

   * Категория А: Задачи на основную бизнес-логику
       * На основе разделов 4 и 5.
       * Мысль: "Какое ядро функциональности мы можем написать и протестировать в полной изоляции, используя моки?"
       * Задачи: Реализовать сервис расчета, Написать маппер данных, Покрыть логику Unit-тестами.

   * Категория Б: Задачи на интеграцию ("клей")
       * На основе разделов 3.2 и 2.1.
       * Мысль: "Какой код нам нужен, чтобы соединить нашу бизнес-логику с реальным миром?"
       * Задачи: Настроить HTTP-клиент с mTLS сертификатом, Добавить использование proxy в клиент, Подключить реальные креды из vault.

   * Категория В: Задачи на эксплуатацию ("броня")
       * На основе раздела 6.
       * Мысль: "Как сделать так, чтобы этот код был надежным и мы знали, что с ним происходит?"
       * Задачи: Реализовать механизм retry, Добавить метрики (счетчики, гистограммы), Настроить детальное логирование.

  Итог

  Ваш подход — это первый, самый важный шаг техлида. Он позволяет выявить все, что может затормозить проект. А моя предыдущая классификация —
  это второй шаг, который структурирует саму разработку.

   * Этап 5: Миграция первого пилотного приложения на новую систему Auth.
       * Тип работы: Интеграция.
       * Результат: Заполненный "Шаблон для интеграции", описывающий, как пилотное приложение будет "общаться" с нашим новым сервисом Auth.
--------------------
Мета-чек-лист: Ключевые вопросы для анализа любой задачи

  1. Цель и Контекст ("Зачем?" и "Кто?")
   * Какую бизнес-проблему мы решаем?
   * Кто является источником данных или участником процесса?
   * С кем из людей нам нужно общаться для получения информации?

  2. Среды и Доступность ("Где это находится?")
   * Существует ли система на PROD и TEST-средах? (ваш пункт)
   * Где она расположена в сети и каков до нее маршрут (прямой, через прокси)?

  3. Доступы и Разрешения ("Как туда попасть?")
   * Есть ли у нас уже доступ к системе, или его нужно получать? (ваш пункт)
   * Если доступа нет, то каков процесс его получения (технический и бюрократический)? (ваш пункт)
   * Нужно ли согласовывать это взаимодействие с кем-то (отдел ИБ, архитекторы)? (ваш пункт)

  4. Данные и Логика ("Что мы делаем?")
   * В каком виде мы получаем данные (технология, формат)?
   * Есть ли у нас примеры этих данных (моки), чтобы начать разработку? (ваш пункт)
   * Что конкретно мы должны сделать с этими данными после получения (наша бизнес-логика)? (ваш пункт)

  5. Эксплуатация и Надежность ("Как сделать, чтобы это работало всегда?")
   * Каковы "правила игры" при работе с системой в промышленной среде (лимиты, политика повторных запросов)?
   * Как мы будем следить, что интеграция работает, и как будем разбирать проблемы (метрики и аудит)?
---------------------
Распределение ролей и ответственности (в виде списка)

  1. Стратегия и Планирование
   * Создание бизнес-целей (Эпики в Jira):
       * Основная роль: Владелец Продукта (PO)
       * Участвующие роли: Техлид, Аналитик
   * Проектирование концептуальной архитектуры (внешние интеграции, схемы в КАФО):
       * Основная роль: Архитектор концептуальной архитектуры
       * Участвующие роли: Владелец Продукта, Аналитик
   * Проектирование Solution-архитектуры (внутреннее устройство сервиса):
       * Основная роль: Solution Architect / Техлид
       * Участвующие роли: DevOps-инженер, Разработчики

  2. Анализ и Проектирование
   * Проведение анализа, заполнение шаблонов:
       * Основная роль: Аналитик ЭД(Вся активнось с внешними источниками+ рекомендации для агента), ИВ(Оля)
      * Проектирование БД(Кирилл)
       * Участвующие роли: Техлид, Команда источника
   * Дизайн UI/UX (создание макетов):
       * Основная роль: UI/UX Дизайнер
       * Участвующие роли: Аналитик, Владелец Продукта

  3. Разработка и Качество
   * Декомпозиция и создание задач на разработку:
       * Основная роль: Техлид
       * Участвующие роли: Разработчики, Аналитик
   * Написание кода и Unit-тестов:
       * Основная роль: Разработчик
       * Участвующие роли: Техлид (для Code Review)
   * Проведение Code Review:
       * Основная роль: Техлид / Другой Разработчик
   * Ручное и автоматизированное тестирование:
       * Основная роль: QA-инженер
       * Участвующие роли: Разработчик, Аналитик
   * Проведение ПСИ (Приемо-сдаточных испытаний):
       * Основная роль: QA-инженер / Владелец Продукта
       * Участвующие роли: Аналитик

  4. Инфраструктура и Доступы
   * Заказ "железа", настройка серверов/ВМ:
       * Основная роль: DevOps-инженер / Системный админ
       * Участвующие роли: Техлид
   * Получение доступов (заявки в ИБ, к системам):
       * Основная роль: Аналитик / Техлид
   * Администрирование серверов (ОС, патчи):
       * Основная роль: Системный администратор (Серега МпР) / DevOps

  5. Релиз и Эксплуатация
   * Настройка CI/CD, мониторинга, логов:
       * Основная роль: DevOps-инженер
       * Участвующие роли: Техлид, Разработчик
   * Подготовка и согласование релиза:
       * Основная роль: Техлид / DevOps-инженер
       * Участвующие роли: Владелец Продукта, QA-инженер
   * Поддержка в PROD, разбор инцидентов:
       * Основная роль: Команда разработки / DevOps (по графику)
---------------------------
 Шаг 2: Декомпозиция самой работы ("Шляпа архитектора")

  На этом этапе техлид перестает думать о том, что может пойти не так, и начинает отвечать на вопрос: "Из каких конкретных технических частей 
  будет состоять наша фича?".

  Он снова смотрит на спецификацию, но теперь уже для того, чтобы спланировать саму разработку. Он мысленно (а затем и в Jira) разделяет всю
  работу на три логические категории.

  Категория А: Задачи на основную бизнес-логику (Ядро функциональности)

  Это "чистый" код, который можно написать и протестировать в полной изоляции, даже не имея никаких доступов.

   * Мысль техлида: "Какой код мы можем написать прямо сейчас, используя только спецификацию и примеры данных (моки)?"
   * На основе: Разделов 4 ("Логика Работы с Данными") и 5 ("Ускорение Разработки").
   * Примеры задач в Jira:
       * Story: [Backend] Реализовать сервис для расчета критерия C-09.
       * Task: Создать DTO (Data Transfer Objects) для ответа от API Системы Х.
       * Task: Написать Unit-тесты для сервиса расчета, используя моки из спецификации.

  Категория Б: Задачи на интеграцию (Технический "клей")

  Это код, который соединяет наше "ядро" с реальным миром. Эти задачи можно взять в работу, когда доступы из Шага 1 будут получены.

   * Мысль техлида: "Какой код нам нужен, чтобы наше 'ядро' смогло реально общаться с внешней системой?"
   * На основе: Разделов 3.2 ("Реквизиты для подключения") и 2.1 ("Сетевое взаимодействие").
   * Примеры задач в Jira:
       * Task: [Backend] Настроить HTTP-клиент с использованием mTLS сертификата из vault.
       * Task: Добавить в конфигурацию клиента использование proxy-сервера.
       * Task: Подключить реальные креды (логин/пароль) из vault для аутентификации.

  Категория В: Задачи на эксплуатацию ("Броня" и "Приборная панель")

  Это код, который делает нашу фичу надежной, наблюдаемой и готовой к работе в промышленной среде.

   * Мысль техлида: "Как сделать этот код отказоустойчивым и как мы узнаем, что с ним что-то не так?"
   * На основе: Раздела 6 ("Эксплуатация и Ограничения").
   * Примеры задач в Jira:
       * Task: [Backend] Реализовать механизм повторных запросов (retry) с экспоненциальной задержкой.
       * Task: [Monitoring] Добавить технические метрики: счетчики успехов/ошибок, гистограмму latency.
       * Task: [Logging] Настроить детальное логирование ключевых шагов интеграции.

  ---

  Итог двух шагов

   * Шаг 1 (Анализ рисков) обеспечивает запуск долгих административных процессов и управление неопределенностью.
   * Шаг 2 (Декомпозиция работы) обеспечивает создание структурированного и понятного бэклога для команды разработки.
-------------------------------
 Шаблон спецификации для новой фичи

  1. Общая информация

   * Название фичи: (Краткое, понятное название, e.g., "Экран модерации расчетов ИЗИ")
   * Ссылка на эпик в Jira: (Ссылка на родительскую задачу)
   * Проблема / Бизнес-цель: (Описать, какую проблему пользователя или бизнеса мы решаем. ЗАЧЕМ мы это делаем? E.g., "Модераторы тратят много
     времени на проверку расчетов, так как у них нет единого интерфейса для просмотра и утверждения. Цель — ускорить процесс модерации в 3
     раза.")
   * Пользовательские роли: (Для КОГО эта фича? E.g., "Модератор", "Администратор")

  2. Функциональные требования в формате User Stories

  (Это основной раздел, описывающий, ЧТО должна делать фича. Лучше всего разбить большую фичу на несколько маленьких историй.)

   * User Story 1: Просмотр списка расчетов
       * Как Модератор, я хочу видеть на экране список всех расчетов, ожидающих модерации, чтобы я мог быстро оценить объем работы.
   * User Story 2: Фильтрация списка
       * Как Модератор, я хочу иметь возможность фильтровать список расчетов по источнику и дате, чтобы я мог найти конкретный расчет.
   * User Story 3: Утверждение/отклонение расчета
       * Как Модератор, я хочу иметь возможность нажать кнопки "Утвердить" или "Отклонить" для каждого расчета, чтобы я мог завершить процесс
         модерации.

  3. Требования к UI/UX (Пользовательский интерфейс)

  (Этот раздел отвечает на вопрос, КАК это должно выглядеть. Он не требует от аналитика быть дизайнером, но требует описать основные
  элементы.)

   * Расположение в интерфейсе: (Где пользователь найдет эту фичу? E.g., "Новый пункт меню 'Модерация' в левой боковой панели.")
   * Основные элементы экрана: (Что пользователь увидит на странице?)
       * Таблица с расчетами. Колонки: ID Источника, Дата расчета, Итоговый балл, Статус.
       * Блок с фильтрами над таблицей: выпадающий список Источник, поле для выбора Даты.
       * В каждой строке таблицы — кнопки Утвердить и Отклонить.
   * Ссылка на макеты/прототипы: (Критически важный пункт!) Ссылка на дизайн в Figma, Balsamiq или даже просто скриншот с нарисованными от руки
     элементами.

  4. Бизнес-логика и правила валидации

  (Здесь описываются конкретные правила и условия.)

   * Кнопка "Утвердить" активна только для расчетов в статусе "Ожидает модерации".
   * При отклонении расчета должно появляться модальное окно с обязательным полем для ввода причины отклонения.
   * Пользователи с ролью "Администратор" видят все расчеты, а "Модератор" — только те, что назначены на его отдел.

  5. Пограничные случаи и обработка ошибок

  (Что будет, если что-то пойдет не так или данных нет?)

   * Пустое состояние: Что показывать, если список расчетов на модерацию пуст? (e.g., "Показывать текст 'Расчетов для модерации нет' и
     картинку.")
   * Ошибка загрузки: Что делать, если не удалось загрузить список с сервера? (e.g., "Показать сообщение об ошибке 'Не удалось загрузить
     данные' и кнопку 'Попробовать снова'.")

  6. Критерии приемки (Acceptance Criteria)

  (Как мы поймем, что фича работает правильно? Формат GIVEN-WHEN-THEN идеален для QA.)

   * Сценарий 1: Успешное утверждение
       * GIVEN: Модератор находится на экране модерации и видит расчет со статусом "Ожидает модерации".
       * WHEN: Он нажимает кнопку "Утвердить".
       * THEN: Статус расчета меняется на "Утвержден", и он пропадает из списка.

  7. Нефункциональные требования

  (Требования к качеству, безопасности, производительности.)

   * Производительность: Список расчетов должен загружаться не дольше 2 секунд.
   * Безопасность: Доступ к этому экрану должен быть только у ролей "Модератор" и "Администратор".
   * Аудит: Каждое действие (утверждение, отклонение) должно записываться в журнал аудита с указанием, какой модератор и когда это сделал.
--------------------
Это отличный и очень важный вопрос, который помогает нам финализировать процесс. Вы все правильно помните, и сейчас мы точно определим место
  тестирования в нашей схеме.

  Короткий ответ: Нет, детальные тест-кейсы в спецификации писать не нужно. Но в спецификации обязательно должен быть раздел "Критерии 
  приемки" (Acceptance Criteria), на основе которого QA-инженер и будет писать свои тест-кейсы.

  Давайте разберем разницу.

  Критерии приемки vs. Тест-кейсы

  Это два разных уровня детализации с разными авторами и целями.

  1. Критерии приемки (Acceptance Criteria)

   * Что это: Высокоуровневые правила, которые описывают, что система должна делать, чтобы фича считалась выполненной. Они отвечают на вопрос:
     "Как мы поймем, что задача решена?".
   * Кто пишет: Аналитик (вместе с Владельцем Продукта). Это неотъемлемая часть спецификации.
   * Формат: GIVEN-WHEN-THEN (Дано-Когда-Тогда), который мы уже используем в шаблоне.
   * Пример (из нашего шаблона):
       * GIVEN: Модератор видит расчет со статусом "Ожидает модерации".
       * WHEN: Он нажимает кнопку "Утвердить".
       * THEN: Статус расчета меняется на "Утвержден".

  2. Тест-кейсы (Test Cases)

   * Что это: Низкоуровневые, пошаговые инструкции для тестировщика, описывающие, как именно проверить функционал. Один критерий приемки
     порождает множество тест-кейсов (позитивных, негативных, на граничные значения).
   * Кто пишет: QA-инженер. Он использует спецификацию и особенно "Критерии приемки" как основу для своей работы. Тест-кейсы обычно пишутся в
     специальных системах (например, TestRail, Zephyr) или просто в Confluence.
   * Формат: Пошаговый скрипт.
   * Пример (на основе критерия приемки выше):
       * Тест-кейс №1 (Позитивный):
           1. Авторизоваться под пользователем moderator_1.
           2. Перейти на страницу /moderation.
           3. Найти в таблице расчет с ID=123. Убедиться, что его статус "Ожидает модерации".
           4. Нажать на кнопку "Утвердить" в строке с ID=123.
           5. Ожидаемый результат: Строка с ID=123 пропадает из таблицы. В базе данных у записи с ID=123 статус изменился на "Утвержден".
       * Тест-кейс №2 (Негативный, проверка прав):
           1. Авторизоваться под пользователем user_simple (у которого нет прав модератора).
           2. Попробовать перейти на страницу /moderation.
           3. Ожидаемый результат: Пользователь видит ошибку "Доступ запрещен" (403).

  Как это выглядит в общем процессе

  Ваш процесс абсолютно верен, давайте просто добавим в него QA-инженера.

   1. ПО создает цель (эпик).
   2. Аналитик создает спецификацию, включая в нее "Критерии приемки".
   3. Техлид декомпозирует спецификацию на задачи для разработчиков.
   4. ПАРАЛЛЕЛЬНО:
       * Разработчики пишут код и юнит-тесты.
       * QA-инженер читает спецификацию и на основе "Критериев приемки" пишет детальные тест-кейсы.
   5. Разработчики заканчивают работу и выкатывают фичу на тестовый стенд.
   6. QA-инженер открывает свои заранее написанные тест-кейсы и по ним проводит тестирование.
   7. ПСИ и релиз на PROD.

  Вывод: Аналитик не пишет детальные инструкции для тестировщика. Он определяет "контракт", по которому фича будет приниматься. А уже
  QA-инженер превращает этот "контракт" в набор исчерпывающих проверок.
------------------
Интеграция — это такая же "фича", просто ее "пользователем" часто является другая система или внутренний процесс. И рисков в ней не меньше,
  а то и больше, потому что проблемы могут быть "тихими" и незаметными сразу.

  Давайте разберем на нашем примере с интеграцией с "Системой Аудит".

  ---

  Критерии приемки (пишет Аналитик в спецификации)

  Аналитик должен добавить в "Шаблон для интеграции" новый раздел, аналогичный тому, что есть в шаблоне для фич.

  Пример Критериев приемки для интеграции с "Системой Аудит":

   * Сценарий 1: Happy Path (инцидент найден)
       * GIVEN: В Системе "Аудит" для источника Source-A существует критичный инцидент.
       * WHEN: Запускается наш сервис расчета индекса ИЗИ.
       * THEN: В нашей базе данных для источника Source-A по критерию "Аудит" сохраняется балл 0 и в поле "доказательство" записывается ID
         этого инцидента.

   * Сценарий 2: Happy Path (инцидентов нет)
       * GIVEN: В Системе "Аудит" для источника Source-B нет критичных инцидентов.
       * WHEN: Запускается наш сервис расчета.
       * THEN: В нашей базе данных для источника Source-B по критерию "Аудит" сохраняется балл 10.

   * Сценарий 3: Обработка ошибки (система-источник недоступна)
       * GIVEN: API Системы "Аудит" не отвечает (возвращает 503 ошибку).
       * WHEN: Запускается наш сервис расчета.
       * THEN: Наш сервис пытается сделать еще 2 повторных запроса (согласно политике retry), и после финальной неудачи в лог записывается
         ошибка уровня ERROR с текстом "Не удалось подключиться к Системе Аудит", а расчет по этому критерию переводится в статус "Ошибка".

  Тест-кейсы (пишет QA-инженер на основе Критериев приемки)

  QA-инженер берет эти критерии и превращает их в детальные технические проверки.

   * Тест-кейс №1.1 (Проверка логики нахождения инцидента):
       1. Поднять мок-сервер, который на запрос GET /api/vincidents?target_system_name=Source-A отдает JSON с критичным инцидентом (из
          спецификации).
       2. Запустить вручную сервис расчета для источника Source-A.
       3. Выполнить SQL-запрос к нашей БД: SELECT score, proof FROM results WHERE source_id='Source-A' AND criterion_id='audit'.
       4. Ожидаемый результат: score равен 0, proof содержит ID инцидента из мок-ответа.

   * Тест-кейс №3.1 (Проверка логики retry при ошибке 503):
       1. Поднять мок-сервер, который на все запросы к GET /api/vincidents отдает HTTP 503 Service Unavailable.
       2. Запустить сервис расчета.
       3. Следить за логами мок-сервера.
       4. Ожидаемый результат: На мок-сервер должно прийти ровно 3 запроса (1 оригинальный + 2 повторных). В логах нашего сервиса должна
          появиться запись об ошибке.
------------------------
Финальный мастер-шаблон для интеграции (с указанием ролей)

  1. Идентификация и Цель
   * С кем консультироваться: Владелец Продукта (PO).
   * Ключевые вопросы:
       * Какую бизнес-проблему мы решаем?
       * Какая система является источником данных?
       * Какие данные нам нужны на концептуальном уровне?
       * С кем со стороны источника велось общение? (контакт для технических вопросов)

  2. Инфраструктура и Окружение
   * С кем консультироваться: Команда источника, DevOps-инженер, Архитекторы (концептуальный и solution).
   * 2.1. Сетевое взаимодействие (Маршрут и Доступы)
       * Ключевые вопросы: Документация архитектуры, Местоположение, Маршрут и Точка подключения, Процесс согласования, Запрос на доступ.
   * 2.2. Тестовые среды
       * Ключевые вопросы: Есть ли у источника тестовый контур? Каковы его реквизиты и ограничения?
   * 2.3. Готовность на PROD
       * Ключевые вопросы: Нужный функционал выведен и стабильно работает в PROD-среде источника?

  3. Доступ к Источнику (Прикладной уровень)
   * С кем консультироваться: Команда источника, Техлид.
   * 3.1. Технология и паттерн интеграции
       * Ключевые вопросы: Какая технология и паттерн используются?
   * 3.2. Реквизиты для подключения (Адреса и Аутентификация)
       * Ключевые вопросы: Адреса/эндпоинты, механизм аутентификации, конкретные креды/ключи.
   * 3.3. Процесс получения доступа (Бюрократия)
       * Ключевые вопросы: Есть ли документация по процессу? Каковы точные шаги?

  4. Логика Работы с Данными
   * С кем консультироваться: Владелец Продукта (для бизнес-смысла), Техлид/Разработчики (для технической реализации логики).
   * Ключевые вопросы:
       * Где находится документация на API / формат сообщений?
       * Какой эндпоинт/топик/таблицу мы используем?
       * Какие параметры запроса нужно передавать / как фильтровать сообщения?
       * Какие поля из ответа/сообщения нам нужны и какую логику к ним применить?
       * Каков финальный результат вычисления?

  5. Критерии приемки (Acceptance Criteria)
   * С кем консультироваться: Владелец Продукта, QA-инженер.
   * Ключевые вопросы:
       * Как мы поймем, что интеграция работает корректно в основном сценарии (happy path)?
       * Как система должна себя вести в случае ожидаемых проблем (e.g., нет данных)?
       * Как система должна обрабатывать технические сбои (e.g., недоступность)?
   * Формат: Рекомендуется использовать GIVEN-WHEN-THEN.

  6. Примеры данных для разработки (моки)
   * С кем консультироваться: Команда источника.
   * Ключевые вопросы:
       * Как выглядят примеры ответов/сообщений для разных сценариев (успех, ошибка, пустой ответ)?

  7. Эксплуатация и Ограничения
   * С кем консультироваться: Команда источника (про лимиты), Техлид и DevOps-инженер (про метрики и ретраи).
   * 7.1. Ограничения на стороне источника
       * Ключевые вопросы: Есть ли rate limits? Какова политика retry?
   * 7.2. Метрики и Аудит
       * Ключевые вопросы: Какие технические метрики нужно собирать? Какие данные нужны для аудита?

  ---
  Теперь шаблон не только говорит аналитику, что нужно узнать, но и к кому пойти, чтобы это узнать. Это делает процесс еще более понятным и
  снижает время на "поиск нужного человека".
-----------------Встречи в рамках двухнедельного спринта

  Всего у вас будет 4 обязательные встречи (церемонии Scrum) и одна очень рекомендуемая.

  1. Планирование спринта (Sprint Planning)

   * Когда: В самом начале спринта (например, в понедельник утром, раз в две недели).
   * Длительность: 2-3 часа.
   * Участники: Вся команда (Владелец Продукта, Техлид, все Разработчики, QA-инженер, Аналитик, DevOps-инженер).
   * Цель:
       1. Владелец Продукта (PO) представляет самые важные задачи из бэклога.
       2. Команда обсуждает эти задачи, задает вопросы аналитику и техлиду.
       3. Команда решает, какой объем работы она реально может взять на ближайшие две недели (формируется "бэклог спринта").
       4. Результат: У команды есть четкая цель на спринт и список задач, которые она обязалась выполнить.

  2. Ежедневный стендап (Daily Stand-up)

   * Когда: Каждый день в одно и то же время (например, в 11:00).
   * Длительность: Строго 15 минут.
   * Участники: Техническая команда (Техлид, Разработчики, QA, DevOps, Аналитик). PO может присутствовать, но не отчитывается.
   * Цель: Быстрая синхронизация, а не решение проблем. Каждый участник отвечает на 3 вопроса:
       1. Что я делал вчера, чтобы помочь команде достичь цели спринта?
       2. Что я буду делать сегодня?
       3. Какие у меня есть проблемы или блокеры?
   * Все обсуждения выносятся за рамки стендапа и решаются после него только с нужными людьми.

  3. Груминг Бэклога (Backlog Refinement/Grooming)
  (Это не официальная церемония Scrum, но это лучшая практика, без которой не обходится почти ни одна команда).

   * Когда: Раз в неделю, обычно в середине спринта (например, в пятницу первой недели).
   * Длительность: 1 час.
   * Участники: Вся команда.
   * Цель: Подготовить задачи для следующего спринта.
       1. PO и аналитик показывают "черновики" будущих задач и спецификаций.
       2. Команда задает первые вопросы, чтобы аналитик мог доработать спецификацию.
       3. Команда дает предварительную, "грубую" оценку сложности (например, по размерам футболок: S, M, L, XL).
   * Это делает следующее Планирование спринта намного быстрее и эффективнее.

  4. Обзор спринта (Sprint Review / Демо)

   * Когда: В конце спринта (например, в пятницу второй недели, перед Ретроспективой).
   * Длительность: 1 час.
   * Участники: Вся команда + приглашенные стейкхолдеры (руководство, пользователи, другие команды).
   * Цель: Показать работающий продукт.
       1. Команда демонстрирует то, что было сделано за спринт. Это живой показ, а не презентация.
       2. Стейкхолдеры дают обратную связь.
       3. PO подводит итоги, что было сделано, а что нет.

  5. Ретроспектива спринта (Sprint Retrospective)

   * Когда: В самом конце спринта, после Демо.
   * Длительность: 1-1.5 часа.
   * Участники: Только команда разработки. Никаких менеджеров и стейкхолдеров со стороны. Это безопасное пространство для команды.
   * Цель: Обсудить процесс работы, а не продукт. Команда отвечает на 3 вопроса:
       1. Что у нас получилось хорошо в этом спринте?
       2. Что можно было бы улучшить?
       3. Какие одно-два конкретных действия мы предпримем в следующем спринте, чтобы стать лучше?
   * Результат: План улучшений на следующий спринт.

  ---

  Расписание встреч на двухнедельный спринт (в виде списка)

  Неделя 1

   * Понедельник (утро):
       * Планирование спринта (длительность 2-3 часа)

   * Каждый день (Вторник - Пятница):
       * Ежедневный стендап (длительность 15 минут)

   * Пятница (середина дня):
       * Груминг Бэклога (длительность 1 час)

  Неделя 2

   * Каждый день (Понедельник - Пятница):
       * Ежедневный стендап (длительность 15 минут)

   * Пятница (конец дня):
       * Обзор спринта / Демо (длительность 1 час)
       * Ретроспектива (длительность 1-1.5 часа)

 Здравствуйте.

  Я с большим интересом ознакомился с вакансией Tech Lead и хотел бы предложить свою кандидатуру. Уверен, что мой практический опыт в проектировании и
  руководстве разработкой сложных AI-систем будет полезен вашей команде.

  В основной работе я совмещаю роли Product Owner и Team/Tech Lead, руковожу внедрением AI-агента. Он рассчитывает «индекс зрелости» источника для загрузки
  данных в корпоративную платформу (КАП). В мои задачи входит: проектирование solution-архитектуры, анализ и построение интеграций для получения данных 
  агентом, декомпозиция задач и контроль полного цикла релиза (ПСИ, прод). Небольшая команда и сжатые сроки требуют активного участия на всех этапах.

  Параллельно, для глубокого изучения AI-систем, в личное время я с нуля создал pet-проект: low-code платформу для AI-автоматизаций на React (Next.js). Она
  имеет drag-and-drop редактор, мульти-командный доступ и хранилище секретов. Ключевой модуль — Dispatcher, AI-оркестратор с Function Calling для вызова
  других воркфлоу. Архитектура расширяется через MCP-сервисы: Ingestion API, Email (IMAP) и RAG-сервис на pgvector.
========================================================================================================================
> в этой задаче только не понятна роль воркера, должен ли воркер выполнять всю работу? я вижу что 
  воркеры у меня по сути это один питон сприпт но который подписывается на разные очередь в постгрес, 
  на соклько это правильно? нужно ли делать отдельный сервис docling?

✦ Отличные уточняющие вопросы! Они касаются ключевых архитектурных решений. Давайте разберем каждый по
  порядку.

  1. Должен ли воркер выполнять всю работу?

  Нет, и в предложенной мной схеме "эфемерной загрузки" он и не выполняет всю работу. Здесь важно
  понять принцип разделения ответственности:

   * Роль Воркера (`ingestion-worker`): Его задача — "добыть" и "подготовить" данные. Он специалист по
     работе с файлами. Он не должен знать, что с этими данными будут делать дальше (отвечать на вопрос,
     суммировать, переводить). Он просто берет источник (URL), извлекает из него чистый текст и говорит:
      "Вот, я достал текст, он готов к работе". В "эфемерном" режиме он кладет этот текст во временное
     хранилище (Redis).

   * Роль Оркестратора (`platform-api` и его воркфлоу): Его задача — дирижировать бизнес-логикой. Он
     знает всю задачу целиком. Он говорит: "Ага, пользователь хочет задать вопрос по файлу. План такой:
     1) Попрошу ingestion-worker достать текст. 2) Когда текст будет готов, возьму его и вопрос
     пользователя. 3) Попрошу GigaChat ответить на вопрос, используя этот текст. 4) Отправлю ответ
     пользователю."

  Такое разделение делает систему гибкой. Завтра вы захотите не отвечать на вопрос, а, например,
  суммировать текст из файла. Вам не придется менять воркер, вы просто измените логику в воркфлоу
  (замените ноду GigaChat на другую). Воркер остается универсальным "добытчиком" текста.

  2. Правильно ли устроен воркер (один скрипт, разные очереди)?

  Ваша текущая реализация с одним скриптом ingestion-worker/main.py и запуском нескольких контейнеров
  для разных очередей (pdf, image и т.д.) — это абсолютно правильный и хороший архитектурный паттерн.

  Вот его преимущества:

   * Изоляция ресурсов и отказоустойчивость: Вы можете независимо масштабировать обработку разных типов
     файлов. Например, если у вас много PDF, вы можете запустить docker-compose up --scale 
     ingestion-worker-pdf=5, не трогая воркеры для docx. Если воркер для обработки изображений упадет
     из-за ошибки в библиотеке, это не затронет обработку других типов файлов.
   * Специализация: В будущем вы сможете запускать воркеры для изображений на машинах с GPU, а для
     текста — на обычных CPU, просто настроив docker-compose.yml для разных окружений.
   * Переиспользование кода: Вся логика обработки находится в одном месте, что упрощает ее поддержку.

  Так что с этой частью у вас все отлично.

  3. Нужно ли делать отдельный сервис для docling?

  Это классический вопрос при проектировании микросервисов: "библиотека или отдельный сервис?". Оба
  подхода имеют право на жизнь.

   * Текущий подход (Docling как библиотека в воркере):
       * Плюсы: Простота. Нет сетевых задержек между воркером и docling, так как все происходит внутри
         одного процесса. Меньше "движущихся частей" в системе.
       * Минусы: Каждый контейнер ingestion-worker "тащит" за собой все тяжелые зависимости (docling,
         torch и т.д.), что делает образ контейнера большим. Если другой сервис захочет использовать
         docling, ему тоже придется все это устанавливать.

   * Альтернатива (Отдельный `docling-service`):
       * Плюсы: Все тяжелые ML-модели и зависимости инкапсулированы в одном месте. Воркеры становятся
         очень "легкими" — им нужно только сделать HTTP-запрос. Этот сервис можно масштабировать
         независимо от всего остального.
       * Минусы: Появляются сетевые издержки (файл нужно передавать по сети). Увеличивается общая
         сложность системы (еще один сервис для поддержки и мониторинга).

  Вердикт: На текущем этапе вашего проекта, использовать `docling` как библиотеку внутри 
  `ingestion-worker` — это абсолютно нормальное и прагматичное решение. Система еще не настолько
  сложна, чтобы оправдать создание отдельного сервиса. Однако, если в будущем вы заметите, что
  обработка файлов становится узким местом или другие сервисы захотят использовать docling, то
  вынесение его в отдельный микросервис будет логичным следующим шагом для масштабирования.

> а не будет так что я запущу один воркер с флагом пдф который использует доклинг а другой воркер с 
  другим флагом котоырй скачивает вебсайт, не получится что зависимости для работы с пдф будут у 
  вебсайта?

✦ Вы задали очень проницательный и важный вопрос. Вы абсолютно правы: в текущей конфигурации воркер для
  веб-сайтов (ingestion-worker-website) будет содержать ненужные ему тяжелые зависимости для обработки
  PDF и изображений (docling, torch и т.д.).

  Это приводит к увеличению размера Docker-образа, более медленному запуску контейнеров и
  неэффективному использованию ресурсов.

  Эту проблему решают с помощью многостадийных сборок (multi-stage builds) в Docker. Это позволяет
  нам использовать один и тот же Dockerfile для создания разных версий образа: "легкой" (только с
  базовыми зависимостями) и "тяжелой" (со всеми ML-библиотеками).

  Как это реализовать

  1. Модифицируем `Dockerfile` в `services/ingestion-worker`

  Мы разделим его на стадии:
   * base: Устанавливает только общие зависимости из requirements.txt.
   * heavy: Наследуется от base и доустанавливает тяжелые пакеты (docling, torch).

    1 # Dockerfile для ingestion-worker с multi-stage build
    2 
    3 # =================================================
    4 # Стадия 1: 'base' - общие зависимости для всех воркеров
    5 # =================================================
    6 FROM python:3.11-slim AS base
    7 
    8 WORKDIR /app
    9 
   10 # Копируем и устанавливаем только легковесные зависимости
   11 COPY requirements.txt .
   12 RUN pip install --no-cache-dir -r requirements.txt
   13 
   14 # Копируем исходный код приложения
   15 COPY . .
   16 
   17 # =================================================
   18 # Стадия 2: 'heavy' - для воркеров, требующих ML-библиотек
   19 # =================================================
   20 FROM base AS heavy
   21 
   22 # Устанавливаем тяжелые зависимости поверх 'base'
   23 RUN pip install --no-cache-dir "docling[vlm]" easyocr torch
   24 
   25 # Загружаем пакеты NLTK, которые нужны для обработки текста
   26 RUN python -m nltk.downloader -d /usr/local/share/nltk_data punkt_tab
   27 
   28 # =================================================
   29 # Стадия 3: 'light' - для легковесных воркеров (просто псевдоним для 'base')
   30 # =================================================
   31 FROM base AS light

  2. Модифицируем `docker-compose.yml`

  Теперь для каждого воркера мы можем указать, какую стадию из Dockerfile использовать с помощью
  ключа target:

    1 # docker-compose.yml (фрагмент)
    2 
    3 services:
    4   # ... другие сервисы
    5 
    6   # --- Воркеры загрузки ---
    7 
    8   # Тяжелый воркер для PDF
    9   ingestion-worker-pdf:
   10     build:
   11       context: ./services/ingestion-worker
   12       target: heavy # <-- Указываем, что нужна "тяжелая" сборка
   13     container_name: my_app_ingestion_worker_pdf
   14     command: python main.py --queue pdf
   15     # ... остальные параметры
   16 
   17   # Тяжелый воркер для изображений
   18   ingestion-worker-image:
   19     build:
   20       context: ./services/ingestion-worker
   21       target: heavy # <-- Указываем, что нужна "тяжелая" сборка
   22     container_name: my_app_ingestion_worker_image
   23     command: python main.py --queue image
   24     # ... остальные параметры
   25 
   26   # Легковесный воркер для сайтов
   27   ingestion-worker-website:
   28     build:
   29       context: ./services/ingestion-worker
   30       target: light # <-- Указываем, что нужна "легкая" сборка
   31     container_name: my_app_ingestion_worker_website
   32     command: python main.py --queue website
   33     # ... остальные параметры
   34 
   35   # Тяжелый воркер для docx
   36   ingestion-worker-docx:
   37     build:
   38       context: ./services/ingestion-worker
   39       target: heavy # <-- Указываем, что нужна "тяжелая" сборка
   40     container_name: my_app_ingestion_worker_docx
   41     command: python main.py --queue docx
   42     # ... остальные параметры
   43 
   44   # Тяжелый воркер для xlsx
   45   ingestion-worker-xlsx:
   46     build:
   47       context: ./services/ingestion-worker
   48       target: heavy # <-- Указываем, что нужна "тяжелая" сборка
   49     container_name: my_app_ingestion_worker_xlsx
   50     command: python main.py --queue xlsx
   51     # ... остальные параметры
   52 
   53   # ... остальные сервисы

  Что мы получаем в итоге:

   * Оптимизированные образы: Контейнеры для website будут "легкими" и быстрыми, без лишних
     зависимостей. Контейнеры для pdf, image и docx будут "тяжелыми", как и требуется.
   * Единый исходный код: Вся логика воркера по-прежнему находится в одной папке (ingestion-worker),
     что удобно для разработки.
   * Гибкость: Вы легко можете добавлять новые типы воркеров ("средние", "супер-тяжелые"), просто
     добавляя новые стадии в Dockerfile.

  Это стандартная и лучшая практика для подобных сценариев. Я могу внести эти изменения в ваш
  Dockerfile и docker-compose.yml, если вы хотите.

> а с точки зрения кода объясни еще раз как это все работает, правильно ли мы написали код что у меня 
  есть ingestion worker а в нем есть loaders различные?

✦ Да, конечно. Код, который вы написали для ingestion-worker и его загрузчиков, организован очень
  правильно и следует хорошим практикам программирования.

  Давайте разберем по шагам, как это работает и почему это хорошая архитектура.

  1. "Мозг" воркера: ingestion-worker/main.py

  Этот файл — точка входа и "мозг" любого воркера, независимо от того, какую очередь он слушает.

   * `main_loop(queue_name)`: Это главный цикл. Он бесконечно делает одно и то же:
       1. Подключается к базе данных.
       2. Ищет одну (LIMIT 1) задачу со статусом pending в своей конкретной очереди (WHERE queue_name = 
          $1).
       3. Ключевой момент: FOR UPDATE SKIP LOCKED — это команда для PostgreSQL, которая говорит: "Найди
          мне задачу, заблокируй ее, чтобы никто другой ее не взял. Если все задачи уже заблокированы
          другими воркерами, просто пропусти их и ищи дальше". Это гарантирует, что два воркера
          (например, два pdf-worker) не начнут обрабатывать один и тот же файл.
       4. Если задача найдена, он вызывает process_job(job, db_pool).
       5. Если задач нет, он "засыпает" на 10 секунд и начинает сначала.

   * `process_job(...)`: Эта функция — оркестратор для одной конкретной задачи. Она не знает, как
     работать с PDF или сайтом, она знает только что нужно сделать в целом:
       1. Извлечь (Extract): Вызвать load_data_from_source(...), чтобы получить сырой текст.
       2. Трансформировать (Transform): Вызвать process_text_to_chunks(...), чтобы нарезать текст и
          получить эмбеддинги.
       3. Загрузить (Load): Сохранить результат в базу данных.

  2. "Диспетчер" загрузчиков: ingestion-worker/loaders/__init__.py

  Это самый важный файл для понимания вашей архитектуры. Он реализует паттерн проектирования
  "Диспетчер" (Dispatcher) или "Стратегия" (Strategy).

    1 # loaders/__init__.py
    2 
    3 # ... импорты конкретных загрузчиков ...
    4 from .website_loader import crawl_website
    5 from .pdf_loader import download_and_parse_pdf
    6 # ...
    7 
    8 async def load_data_from_source(source_type: str, url: str, **kwargs) -> str:
    9     """
   10     Главная функция-диспетчер.
   11     """
   12     if source_type == 'website':
   13         return await crawl_website(url, **kwargs) # --> Вызываем специалиста по сайтам
   14     elif source_type == 'pdf':
   15         return await download_and_parse_pdf(url, **kwargs) # --> Вызываем специалиста по 
      PDF
   16     # ... и так далее

  Как это работает:
   * "Мозг" (main.py) не знает, как обрабатывать PDF. Он просто говорит диспетчеру: "Мне нужно
     обработать источник типа pdf по этому url".
   * Диспетчер (load_data_from_source) смотрит на source_type и, как секретарь, направляет вызов
     правильному "специалисту".
   * Преимущество: Основной код воркера (main.py) остается чистым и не зависит от деталей реализации
     каждого загрузчика. Если вы захотите добавить поддержку нового формата (например, .epub), вам
     нужно будет только добавить файл epub_loader.py и одну строчку elif source_type == 'epub': ... в
     диспетчер, не трогая основной код.

  3. "Специалисты": файлы в ingestion-worker/loaders/

  Каждый файл в этой папке — это узкопрофильный специалист.

   * website_loader.py: Знает, что для сайтов нужно использовать aiohttp для скачивания и markdownify
     для очистки HTML.
   * pdf_loader.py: Знает, что для PDF нужно использовать docling с VLM-пайплайном.
   * generic_docling_loader.py: Знает, как работать с форматами, которые docling понимает "из коробки"
     (docx, xlsx).

  Эта структура идеальна, потому что каждый специалист занимается только своим делом.

  Итог: Правильно ли это?

  Да, это отличная, чистая и масштабируемая архитектура.

  Можно провести аналогию:
   * main.py — это менеджер проекта.
   * loaders/__init__.py — это его секретарь-диспетчер.
   * Каждый файл в loaders — это узкопрофильный специалист (один по PDF, другой по сайтам).

  Менеджеру не нужно знать, как именно работает специалист по PDF. Ему достаточно дать задачу
  секретарю ("обработай вот этот PDF"), который направит ее нужному исполнителю. Это позволяет легко
  заменять специалистов или добавлять новых, не переучивая менеджера.

  Вы интуитивно реализовали очень правильный и профессиональный подход к организации кода.
------------------

[ВАШЕ ИМЯ]
  Tech Lead (LLM & AI Platforms)

  [Контактная информация: Телефон, Email, LinkedIn, GitHub, Город]

  ---

  Обо мне (Summary)

  Опытный Tech Lead и архитектор IT-решений с фокусом на проектировании и внедрении сложных систем с
  применением LLM. Специализируюсь на создании масштабируемых микросервисных архитектур на
  Python/FastAPI и обладаю практическим опытом full-stack разработки (Next.js/React). Успешно
  руковожу командой, проектирую data-пайплайны для AI-продуктов и управляю полным циклом разработки
  от идеи до внедрения.

  ---

  Ключевые навыки (Skills)

   * Архитектура и Управление: Микросервисная архитектура, Solution Architecture, Проектирование API
     (REST, JSON-RPC/MCP), Управление командой, Декомпозиция задач, Agile.
   * AI / ML: RAG (Retrieval-Augmented Generation), LLM-интеграция, Function Calling, Prompt
     Engineering, AI-агенты, pgvector, Docling.
   * Backend: Python, FastAPI, aiohttp, asyncpg, PostgreSQL, Docker, OpenShift.
   * Frontend: TypeScript, Next.js, React, Tailwind CSS, Server-Sent Events (SSE).
   * DevOps: CI/CD, ArgoCD, Git.

  ---

  ОПЫТ РАБОТЫ

  [Название компании] | г. [Ваш город]

  Tech Lead (AI Platform & Data Integration)
  (Январь 2025 – Настоящее время)

  Руковожу созданием платформы "Индекс зрелости источника", построенной на микросервисной архитектуре
  в OpenShift. Платформа автоматически оценивает готовность источников данных к интеграции с помощью
  AI.

   * Архитектура и разработка платформы: Отвечаю за полный цикл разработки ключевых модулей системы:
       * Backend (Python/FastAPI): Проектирование и реализация сервиса, который осуществляет сбор,
         обработку и подготовку данных для AI-модели, а также обеспечивает авторизацию, логирование и
         мониторинг.
       * Frontend (Next.js/React): Разработка пользовательского интерфейса для отображения результатов
         анализа и рекомендаций.
   * Техническое лидерство и интеграция AI:
       * Выступаю в роли технического заказчика и архитектора интеграции для модуля AI-агентов,
         разрабатываемого смежной командой.
       * Определяю функциональные требования к AI-агенту ("что он должен уметь"), проектирую
         API-контракты для взаимодействия и контролирую его бесшовную интеграцию в общую платформу.
   * Управление командой и процессами:
       * Декомпозирую бизнес-требования на технические задачи для команды разработки, провожу их оценку
         и планирую бэклог.
       * Контролирую процессы CI/CD (ArgoCD), прохождения ПСИ (приемо-сдаточных испытаний) и задачи по
         администрированию серверов.

  Ведущий разработчик / Team Lead
  (Январь 2024 – Декабрь 2024)

  Выполнял гибридную роль Team Lead и Full-stack разработчика в команде из 4 человек при создании
  внутренней платформы "FP Mars".

   * Full-stack разработка: В условиях отсутствия frontend-разработчика самостоятельно с нуля за месяц 
     освоил React/Next.js и взял на себя всю frontend-часть проекта.
   * Разработка продукта с нуля: Руководил созданием и лично участвовал в разработке двух ключевых
     модулей платформы:
       1. Дашборд дорожных карт: Интерактивный дашборд для внешних команд для отслеживания прогресса
          интеграции с корпоративной платформой.
       2. Система отчетности: Модуль для внутреннего использования в управлении, агрегирующий данные по
          загрузкам с возможностью выгрузки в Excel.

  Специалист по интеграции данных
  (2022 – 2023)

   * Консультировал команды-источники по вопросам интеграции с корпоративной аналитической платформой
     (КАП), проводил технический аудит конфигураций и помогал устранять ошибки, сокращая время
     подключения новых источников.

  ---

  ЛИЧНЫЕ ПРОЕКТЫ

  Платформа для автоматизации AI-воркфлоу (Low-Code) | [github.com/your-link]

  Спроектировал и с нуля разработал full-stack платформу для визуальной оркестрации AI-процессов.

   * Проектирование архитектуры: Спроектировал и реализовал микросервисную, событийно-ориентированную 
     архитектуру на Python (FastAPI) и Docker, обеспечившую масштабируемость и отказоустойчивость
     компонентов (API, RAG-сервис, воркеры).

   * Разработка RAG-пайплайна: Разработал end-to-end RAG-систему для семантического поиска по базе
     знаний.
       * Реализовал гибридный поиск: быстрый векторный поиск на PostgreSQL + pgvector и последующее
         LLM-переранжирование с использованием Function Calling для повышения релевантности контекста.
       * Спроектировал отказоустойчивый ETL-пайплайн для асинхронной обработки и векторизации
         неструктурированных данных (PDF, DOCX, изображения) с помощью воркеров и системы очередей.

   * Внедрение Agentic Workflows: Спроектировал и реализовал ключевой компонент платформы —
     ноду-оркестратор, реализующую паттерн AI-агента. Агент способен динамически строить план из
     нескольких воркфлоу и последовательно вызывать их для решения сложных, многоэтапных задач.

   * Оптимизация и масштабирование:
       * Внедрил инженерную практику многостадийных сборок Docker (multi-stage builds) для оптимизации
         размеров "легковесных" и "тяжелых" (ML) образов воркеров.
       * Разработал и внедрил единый протокол взаимодействия между сервисами на базе JSON-RPC (Model 
         Context Protocol, MCP), что упростило интеграцию и стандартизировало их общение.

   * Стек технологий:
       * Backend: Python, FastAPI, PostgreSQL, pgvector, Docker, aiohttp, asyncpg, SSE.
       * Frontend: TypeScript, Next.js, React, Tailwind CSS.
       * AI/ML: GigaChat API (Embeddings, Chat, Function Calling), NLTK, Docling.
--------------------
5. Тест-кейсы

  TC-1: Проверка точности цитирования данных (Happy Path)
   * Цель: Убедиться, что агент правильно "зачитывает" цифры и факты, полученные от бэкенда.
   * Действие: Запросить у нового агента расчет для "Частичного источника".
   * Ожидаемый результат: Итоговый балл и детализация по критериям (К1, К2, К3) в тексте ответа агента
     полностью совпадают с данными, которые вернул новый эндпоинт бэкенда.

  TC-2: Проверка поведения при идеальном результате
   * Цель: Убедиться, что агент корректно обрабатывает случай, когда все критерии выполнены.
   * Действие: Запросить у нового агента расчет для "Идеального источника".
   * Ожидаемый результат: Агент сообщает о максимальном балле и об отсутствии необходимости в
     рекомендациях, формируя логичный и законченный ответ (например, "Поздравляю, ваш источник
     полностью соответствует требованиям!").

  TC-3: Сравнение конкретности и полезности рекомендаций (Критический тест)
   * Цель: Доказать, что агент способен преобразовать структурированную "причину проблемы" от бэкенда в
     конкретный, полезный совет для пользователя, не уступающий по качеству совету от старого агента.
   * Проверяемое поведение агента:
       1. Агент получает от бэкенда JSON с детализацией, например: ... "details": {"K2": {"status": 
          "failed", "missing_components": ["installation_section"]}} ...
       2. Агент идентифицирует проблему: не выполнен критерий К2.
       3. Агент анализирует причину: поле missing_components указывает на отсутствие секции установки.
       4. Агент генерирует рекомендацию, которая напрямую решает эту конкретную проблему. Он не должен
          давать общих советов.
   * Действие: Запросить у нового агента расчет для "Плохого источника" и "Частичного источника".
   * Ожидаемый результат (Критерии оценки):
       * Пример УСПЕШНОГО теста (Качество сохранено):
           * Эталон (Старый агент): "Рекомендация: Добавьте в ваш файл README.md раздел ## Установка."
           * Новый агент: "Рекомендация: Чтобы исправить проблему с документацией, вам необходимо
             добавить раздел ## Установка в файл README.md."
           * *Вердикт: Успех. Совет конкретный и решает ту же проблему, что и эталон.*
       * Пример ПРОВАЛЬНОГО теста (Качество деградировало):
           * Эталон (Старый агент): "Рекомендация: Добавьте в ваш файл README.md раздел ## Установка."
           * Новый агент: "Рекомендация: Вам следует улучшить вашу документацию."
           * *Вердикт: Провал. Совет стал абстрактным и бесполезным. Агент не смог правильно
             использовать детализацию от бэкенда.*

  TC-4: Проверка отказоустойчивости и обработки ошибок API
   * Цель: Проверить, что агент вежливо обрабатывает сбои в работе бэкенда.
   * TC-4.1: Бэкенд-сервис недоступен
       * Действие: Эмулировать сбой эндпоинта расчетов (возврат ошибки 5xx или таймаут).
       * Ожидаемый результат: Агент выдает вежливое сообщение о временной недоступности сервиса, не
         показывая пользователю технические детали ошибки.
   * TC-4.2: Бэкенд вернул неполные данные
       * Действие: Эмулировать успешный ответ от бэкенда, но с отсутствующим полем детализации
         (details).
       * Ожидаемый результат: Агент сообщает ту информацию, которая у него есть (например, итоговый
         балл), и корректно информирует о невозможности предоставить детализацию и рекомендации.

  TC-5: Регрессионный тест "отключения"
   * Цель: Доказать, что агент полностью переключился на новый API.
   * Действие: Временно заблокировать доступ агента к старому эндпоинту, который отдавал сырые данные.
     Повторно выполнить TC-1 и TC-3.
   * Ожидаемый результат: Все тесты проходят успешно, подтверждая, что агент больше не зависит от
     старого API.

  TC-6: Проверка контекстной осведомленности (Уточняющие вопросы)
   * Цель: Проверить, может ли агент вести осмысленный диалог о результатах.
   * Действие: После получения ответа по TC-1 задать уточняющий вопрос: "Расскажи подробнее, почему
     критерий К2 не выполнен?".
   * Ожидаемый результат: Агент использует детализацию, полученную от бэкенда, для формирования
     релевантного ответа на уточняющий вопрос.

  ---

  6. Критерии успеха

  Проект по рефакторингу считается успешным, если:
   1. Все тест-кейсы (TC-1 – TC-6) выполнены и их фактические результаты соответствуют ожидаемым.
   2. Качество рекомендаций в TC-3 признано не хуже эталонного.
   3. Агент демонстрирует отказоустойчивость в TC-4.
